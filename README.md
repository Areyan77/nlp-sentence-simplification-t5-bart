# Simplifying Sentences with Small Transformers: A Comparative Study of T5 and BART

A Natural Language Processing (NLP) research project evaluating the performance of **T5-small** and **BART-base** for English sentence-level text simplification.

## ğŸ“˜ Overview

This project fine-tuned two compact transformer models (T5-small, BART-base) using the WikiLarge dataset on Google Colab. The goal: enhance accessibility by simplifying complex sentences.

## ğŸš€ Models & Tools

- ğŸ¤– T5-small (Text-to-Text)
- ğŸ¤– BART-base (Denoising Autoencoder)
- ğŸ“š Dataset: WikiLarge
- ğŸ›  Tools: Hugging Face Transformers, PyTorch, Colab (T4 GPU)

## ğŸ“Š Evaluation Metrics

| Metric    | T5-small | BART-base |
|-----------|----------|-----------|
| BLEU      | 0.2652   | 0.3087    |
| ROUGE-1   | 0.5952   | 0.6208    |
| SARI      | 36.13    | 38.25     |
| FKGL      | 10.02    | 10.09     |

## ğŸ“„ Contents

- ğŸ“ `docs/NLP_Project_Report.pdf` â€” full research report
- ğŸ““ `notebooks/NLP_Project.ipynb` â€” training and evaluation
- ğŸ§ª Evaluation scripts using BLEU, ROUGE, SARI, FKGL
- ğŸ“¦ Fine-tuned models (optional, hosted or linked)

## ğŸ’¡ Findings

- **BART-base** outperformed T5-small across most metrics.
- **T5-small** required less GPU and trained fasterâ€”suitable for low-resource devices.
- Both models produced outputs around 10th-grade reading level â€” still complex for intended audience.

## ğŸ“Œ Authors

- Ugyen Tshering
- Abdullahi Adewole Zakariyah
- Nima Yoezer
- Areyan Muhemed Ali Hussein

